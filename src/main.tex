\documentclass[10pt]{beamer}
\usetheme{metropolis}
% all imports
\input{all_imports}

\AtBeginEnvironment{quote}{\singlespacing}

% new commands
\input{all_new_commands}

% definitions
\input{definitions/colors}
\input{definitions/styles}

\input{header}

\begin{document}
\nocite{DeepLearningbook}
\maketitle

\section{Conceitos básicos}

\begin{frame}{Introdução a aprendizado de máquina}
\input{tikzfiles/MLintro}
\end{frame}

\begin{frame}{Regressão}
\begin{center}
\includegraphics[scale=0.40]{images/house_prices1.png}
\end{center}
\end{frame}

\begin{frame}{Regressão}
\begin{center}
\includegraphics[scale=0.40]{images/house_prices2.png}
\end{center}
\end{frame}


\begin{frame}{Classificação: Fashion MNIST \cite{xiao2017/online}}
\input{tikzfiles/fashionMNIST}
\end{frame}


\begin{frame}{Aprendizado por redes neurais}

O que sabemos sobre o cérebro:
\vspace{0.2cm}
\begin{itemize}
\item neurônios em rede
\vspace{0.2cm}
\item  neurônios emitem sinais elétricos (disparam)
\vspace{0.2cm}
\item dendritos e axônios


\end{itemize}

\includegraphics[width=.50\textwidth]{images/neuron.png}
\includegraphics[width=.50\textwidth]{images/neuronsnetwork.png}
\\
\\
\footnotesize{imagem retirada de \cite{metodosupera}}


\end{frame}



\begin{frame}{Aprendizado por Redes Neurais}

Traduzindo para o modelo de redes neurais artificiais:
\vspace{0.2cm}
\begin{itemize}
\item Energia recebida: \alert{Inputs}
\vspace{0.2cm}
\item  Energia enviada: \alert{Output}
\vspace{0.2cm}
\item  Força entre uma conexão e outra: \alert{Peso ($\vect{\theta}$)}
\vspace{0.2cm}

\item Carga mínima: \alert{Threshold}
\vspace{0.2cm}
\item Uma função que recebe um input e emite um output mas leva em consideração um threshold mínimo: \alert{Função de ativação}
\end{itemize}
\end{frame}


\begin{frame}{Função de ativação: função sigmoide}
\input{tikzfiles/Sigmoid}
\end{frame}

\begin{frame}{função de ativação: ReLU (Rectified Linear Unit)}
\input{tikzfiles/ReLU}
\end{frame}

\begin{frame}[fragile]{Perceptron}
\input{tikzfiles/perceptron1}
\end{frame}

\begin{frame}[fragile]{Perceptron}
\input{tikzfiles/perceptron}
\end{frame}

\begin{frame}[fragile]{Multi layer perceptron -- Feedforward neural network}
\input{tikzfiles/MLP}
\end{frame}

\begin{frame}[fragile]{Multi layer perceptron -- Feedforward neural network}
\input{tikzfiles/MLP2}
\end{frame}

\begin{frame}[fragile]{Multi layer perceptron -- Feedforward neural network}
\input{tikzfiles/MLP3}
\end{frame}

\begin{frame}[fragile]{Multi layer perceptron -- Feedforward neural network}
\input{tikzfiles/MLP4}
\end{frame}

\begin{frame}[fragile]{Deep feedforward network}
\input{tikzfiles/DeepNetwork}
\end{frame}


\begin{frame}[fragile]{Features}
\textbf{\alert{Features}} são características ou traços do objeto do aprendizado.
\\
\visible<2->{\\O aprendizado irá acontecer a partir do \textbf{reconhecimento de padrões} entre \textbf{features}.}
\\
\visible<3->{\\Treinaremos o modelo a \textbf{ativar as mesmas unidades simultaneamente} quando diante de um \textbf{determinado padrão}.}
\end{frame}

\begin{frame}[fragile]{Classificação de imagens}

\input{tikzfiles/hidden_layer}
\end{frame}

\begin{frame}[fragile]{Classificação de imagens}
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{images/wolf.png}
\end{figure}
\footnotesize{imagem retirada de \cite{Ribeiro:2016:WIT:2939672.2939778}}
\end{frame}


\begin{frame}[fragile]{Feedforward neural network  (rede neural)}
\input{tikzfiles/NN1}
\end{frame}

\begin{frame}[fragile]{Versão resumida de uma rede neural}
\input{tikzfiles/CompGraph}
\end{frame}

\begin{frame}{Treinamento}
\input{tikzfiles/ideageral}
\end{frame}


\begin{frame}[fragile]{Descida do gradiente}
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{images/Minimization_image.png}
\end{figure}
\end{frame}

\section{Aprendizado de máquina em linguística}

\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}

Problema:
\begin{itemize}
\item Aprendizado dos verbos "irregulares" no passado do inglês
\vspace{0.2cm}
\visible<2->{\item Famílias de verbos irregulares:}
\vspace{0.1cm}
	\visible<3->{\begin{itemize}
	\item “blow–blew, grow–grew, know–knew, throw–threw”
	\item “bind–bound, find–found, grind–ground, wind–wound”
	\item “drink–drank, shrink–shrank, sink–sank, stink–stank”}
	\end{itemize}
\vspace{0.2cm}
\visible<4->{\item Chomsky vs Rumelhart e McClelland}
\vspace{0.2cm}
\visible<5->{\item \alert{Regras} (Racionalismo) vs  \alert{Analogias} (Conexionismo)}
\end{itemize}

\end{frame}


\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}

Exemplo de entrada $\vect{x}$ e saída $\vect{y}$:

\begin{itemize}
\item [] $(\vect{x}^{(1)}, \vect{y}^{(1)}) =$ (begin, began).
\item [] $(\vect{x}^{(2)}, \vect{y}^{(2)}) =$ (love, loved)
\item [] $(\vect{x}^{(3)}, \vect{y}^{(3)}) =$ (drink, drank)
\item [] $(\vect{x}^{(4)}, \vect{y}^{(4)}) =$ (hate, hated)
\item [] $(\vect{x}^{(5)}, \vect{y}^{(5)}) =$ (grow, grew)
\item [] $(\vect{x}^{(6)}, \vect{y}^{(6)}) =$ (bind, bound)
\item [] $(\vect{x}^{(7)}, \vect{y}^{(7)}) =$ (hit, hit)
\item [] $\dots$
\end{itemize}
\end{frame}


\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}

$\vect{x}$ será uma combinação de traços (features) fonológicos.

\begin{table}[]
\centering
\caption{Categorização de Fonemas em 4 dimensões simples}
\label{fontable}
\begin{tabular}{llllllll}
\hline
 &  & \multicolumn{6}{c}{Place} \\ \hline
 &  & \multicolumn{2}{c|}{Front} & \multicolumn{2}{c|}{Middle} & \multicolumn{2}{c}{Back} \\ \hline
 &  & \multicolumn{1}{l|}{V/L} & \multicolumn{1}{l|}{U/S} & \multicolumn{1}{l|}{V/L} & \multicolumn{1}{l|}{U/S} & \multicolumn{1}{l|}{V/L} & U/S \\ \hline
 \multicolumn{1}{c}{\multirow{2}{*}{Int.}} & Stop & b & p & d & t & g & k \\ \cline{2-8} 
\multicolumn{1}{c}{} & Nasal & m & - & n & - & N & - \\ \hline
\multirow{2}{*}{Cont} & Fric & v/D & f/T & z & s & Z/j & S/C \\ \cline{2-8} 
 & Liq/SV & w/l & - & r & - & y & h \\ \hline
 \multirow{2}{*}{Vowel} & High & E & i & O & $\hat{}$ & U & u \\ \cline{2-8} 
 & Low & A & e & I & a/$\alpha$ & W & o \\ \hline
\end{tabular}
\end{table}

\end{frame}

\begin{frame}[fragile]{Explicando as features}
Exemplo: begin (bEgiN)
\vspace{1cm}
\\
\input{tikzfiles/lingfeatures}

\end{frame}

\begin{frame}[fragile]{Wickelfeatures}

Wickelfeatures: Trigramas de features
\vspace{0.4cm}
\\Exemplo: begin
\input{tikzfiles/wickelfeatures}
\end{frame}



\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}
\input{tikzfiles/phon_vector}
\end{frame}


\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}
\input{tikzfiles/phon_output}
\end{frame}

\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}
\input{tikzfiles/MSE}
\end{frame}


\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}
Resultados (PROS):

\begin{itemize}
\visible<2->{\item Identificou padrões corretamente entre todos os 420 verbos do treinamento;}
\vspace{0.4cm}
\visible<3->{\item Taxa de acerto de \textbf{92\%} para verbos regulares ausentes no treinamento;}
\vspace{0.4cm}
\visible<4->{\item Taxa de acerto de \textbf{84\%} para verbos irregulares ausentes no treinamento;}
\vspace{0.4cm}
\visible<5->{\item \textbf{U-shaped Development}}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{On Learning the Past Tenses of English Verbs}
Resultados (CONS):

\begin{itemize}
\visible<2->{\item Falha ao tentar fazer predições com palavras que compartilham muitas features em comum; \\Exemplo: "algalgal" - Oykangand}
\vspace{0.4cm}
\visible<3->{\item Problemas como uma teoria da mente}
\end{itemize}
\end{frame}



\begin{frame}[fragile]{As Irregularidades no Português Brasileiro}
Desafios:
\begin{itemize}
\visible<2->{\item "Wug Test"; \\ Exemplos: "poguir", "redir", "atover"}
\vspace{0.2cm}
\visible<3->{\item Adaptar a rede para a língua portuguesa}
\vspace{0.2cm}
\visible<4->{\item Melhorar o desempenho da rede}
\end{itemize}
\end{frame}

\section{Modelos de linguagem e redes recorrentes}


\begin{frame}{Definition}
Nos chamamos de \alert{modelo de linguagem} uma distribuição de probabildiade sobre uma sequencia de tokens em uma lingua natural.

\[
P(x_1,x_2,x_3,x_4) = p
\]

\textbf{Usamos esse modelo em}:
\begin{itemize}
\item reconhecimento de fala
\item tradução automática
\item text auto-completion
\item correção de texto
\item resposta automatizada
\item sumarização
\end{itemize}


\end{frame}

\begin{frame}{Revisão de probabilidade}
\large{
\begin{itemize}
\item \textbf{Probabilidade condicional} 
\[
P(A|B) = \frac{P(A,B)}{P(B)}
\]
\item \textbf{Independência} 
\[
P(A|B) = P(A)
\]
\item \textbf{Regra da cadeia} 
\[
P(A,B,C) = P(A)P(B|A)P(C|A,B)
\]
\end{itemize}
}
\end{frame}


\begin{frame}{Como calculamos essas probabilidades?}
Regra da cadeia:

\begin{equation*}
P(x_1,x_2,x_3,x_4) = P(x_1)P(x_2\vert x_1)P(x_3\vert x_1x_2)P(x_4\vert x_1x_2x_3)
\end{equation*}

\vspace{0.3cm}

Para simplificação fazemos uma \textbf{suposição de Markov}, i.e., para um  $n$ específico assumimos certas independências, assim cada palavra depende apenas das últimas $n-1$ palavras:



\begin{equation*}
P(x_1, \dots, x_T) = \prod_{t=1}^{T} P(x_t \vert x_1, \dots, x_{t-1}) = \prod_{t=1}^{T} P(x_{t} \vert x_{t - (n+1)}, \dots, x_{t-1})
\end{equation*} 

\end{frame}

\begin{frame}{Modelos baseados em estatísticas de $n$-gramas}
A escolha de $n$ leva a modelos diferentes.\\

\textbf{Modelo de unigrama} ($n=1$): 
\begin{equation*}
P_{uni}(x_1, x_2, x_3, x_4) = P(x_1)P(x_2)P(x_3)P(x_4)
\end{equation*}

em que $P(x_i) = count(x_i)$.\\

\textbf{Modelo de bigrama} ($n=2$): 
\begin{equation*}
P_{bi}(x_1,x_2,x_3,x_4) = P(x_1)P(x_2\vert x_1)P(x_3\vert x_2)P(x_4\vert x_3)
\end{equation*} 
em que
\[
P(x_i\vert x_j) = \frac{count(x_i, x_j)}{count(x_j)}
\]
\end{frame}

\begin{frame}{Estatísticas de $n$-gramas}
\url{https://books.google.com/ngrams}
\vspace{0.4cm}

\includegraphics[scale=0.14]{images/AI_ML.png}
\end{frame}



\begin{frame}{Modelos baseados em estatísticas de $n$-gramas}
\begin{itemize}
\item Quanto maior o $n$ melhor a performance do modelo.
\vspace{0.7cm}
\item Quanto maior o $n$ maior o uso de memória!
\vspace{0.1cm}
\begin{quote}
"Using one machine \textbf{with 140 GB
RAM for 2.8 days}, we built an unpruned
model on 126 billion tokens."
\end{quote}
\begin{itemize}
\item [] \textit{Scalable Modified Kneser-Ney Language Model Estimation} by Heafield et al.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Modelos de linguagem como predição de data sequencial}

Em vez de usar uma abordagem que seja específica para o domínio da linguagem natural, podemos usar um modelo para predição de dados sequencias:  \textbf{uma rede recorrente (RNN)}. \\

Nossa tarefa de aprendizado é estimar a distribuição de probabilidade

\[
P(x_{n} = \text{palavra}_{j^{*}} | x_{1}, \dots ,x_{n-1})
\]

para qualquer $(n-1)$ sequencia de palavras $x_{1}, \dots ,x_{n-1}$.
\end{frame}

\begin{frame}{Classificação com uma deep feedforward network}
\input{tikzfiles/DFNclassification}
\end{frame}

\begin{frame}{Classificação com uma RNN}
\input{tikzfiles/RNNclassification}
\end{frame}

\begin{frame}{Classificação com uma RNN}
\input{tikzfiles/RNNsentiment}
\end{frame}


\begin{frame}[fragile]{RNNs}
\input{tikzfiles/RNN1}
\end{frame}


\begin{frame}[fragile]{RNNs}
\input{tikzfiles/RNN2}
\end{frame}



\begin{frame}{Exemplo de um dataset}
Nos separamos um corpus $C$ com $T$ tokens e vocabulário $\Vocab$.\\\

Exemplo: \textbf{Make Some Noise}, the Beastie Boys.\\

\begin{quote}
\alert{Yes, here we go again, give you more, nothing lesser\\
Back on the mic is the anti-depressor\\
Ad-Rock, the pressure, yes, we need this\\
The best is yet to come, and yes, believe this\\
... \\}
\end{quote}

\begin{itemize}
\item $T = 378$
\item $|\Vocab| = 186$
\end{itemize}

\end{frame}

\begin{frame}{Exemplo de um dataset}
O dataset é uma coleção de pares $(\vect{x},\vect{y})$ em que $\vect{x}$ é uma palavra e $\vect{y}$ é a palavra imediatamente a direita. Por exemplo:
\begin{itemize}
\item [] $(\vect{x}^{(1)}, \vect{y}^{(1)}) =$ (Yes, here).
\item [] $(\vect{x}^{(2)}, \vect{y}^{(2)}) =$ (here, we)
\item [] $(\vect{x}^{(3)}, \vect{y}^{(3)}) =$ (we, go)
\item [] $(\vect{x}^{(4)}, \vect{y}^{(4)}) =$ (go, again)
\item [] $(\vect{x}^{(5)}, \vect{y}^{(5)}) =$ (again, give)
\item [] $(\vect{x}^{(6)}, \vect{y}^{(6)}) =$ (give, you)
\item [] $(\vect{x}^{(7)}, \vect{y}^{(7)}) =$ (you, more)
\item [] $\dots$
\end{itemize}
\end{frame}


\begin{frame}{O modelo de linguagem com RNN}
\input{tikzfiles/LanguageModelSimplified}
\end{frame}

\begin{frame}{Word Embeddings}
\input{tikzfiles/embedding_vector}
\end{frame}


\begin{frame}{O modelo de linguagem com RNN}
\input{tikzfiles/LanguageModelUnfolded}
\end{frame}

\begin{frame}{Dificuldades ao se treinar uma RNN}

\begin{itemize}
\item Quando nos inicializamos $\vect{W}$ de modo que $||\vect{W}|| < 1$, os gradientes dos passos mais antigos vão sumir (\alert{vanishing problem}).
\vspace{0.2cm}
\item E quando $||\vect{W}|| > 1$, os gradientes dos passos mais antigos vão explodir (\alert{exploding problem}).
\vspace{0.2cm}
\item Como resultado os gradientes dos passos mais próximos ao passo final vão ter mais influência do que os passos mais distantes. Isso é ruim para capturar \alert{longas dependências}.
\vspace{0.2cm}
\begin{itemize}
\visible<2->{\item[] Eu fui morar na \textbf{frança}, com uma $\_\_\_\_\_\_\_$.} \visible<3->{\textbf{(francesa)}}
\vspace{0.3cm}
\visible<4->{\item[] Eu fui morar na \textbf{frança}, nesse tempo fiquei estudando a língua $\_\_\_\_\_\_\_$.} \visible<5->{\textbf{(francesa)}}
\vspace{0.3cm}
\visible<6->{\item[] Eu fui morar na \textbf{frança} durante três anos e cinco meses com dois amigos, o Carlos e o Lucas. Foi bem legal, nesse tempo fiquei estudando a língua $\_\_\_\_\_\_\_$.} \visible<7->{\textbf{(francesa)}}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Algumas soluções}
\begin{itemize}
\item GRU (Gated Recurrent Unit)
\vspace{0.2cm}
\item LSTM (Long Short-Term Memory).
\end{itemize}
\vspace{0.2cm}
\input{tikzfiles/LSTM}
\end{frame}

\begin{frame}{Exemplo de aplicação: TrumpBot}
\url{https://github.com/felipessalvatore/MyTwitterBot}
\begin{center}
\includegraphics[scale=0.24]{images/TrumpBot.png}
\end{center}
\end{frame}

\begin{frame}{Exemplo de aplicação: SakaBot}
\url{https://github.com/felipessalvatore/MyTwitterBot}
\begin{center}
\includegraphics[scale=0.24]{images/SakaBot.png}
\end{center}
\end{frame}


\begin{frame}{Tradução automática: Inglês $\rightarrow$ SPARQL\cite{luzfinger2017}}
\input{tikzfiles/Translation}
\end{frame}


\begin{frame}{LSTM e GRU são apenas o começo}
\textbf{Perplexidade} é uma medida de quantas palavras diferentes igualmente prováveis podem seguir uma sequencia de palavras (pior caso $|\Vocab|$, melhor caso $1$). 

\vspace{0.5cm}

Olhando o corpus \alert{Penn Treebank (PTB)} ($|\Vocab| = 10000$):

\vspace{0.5cm}

\begin{figure}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\cellcolor{blue!60}Model & \cellcolor{blue!60}Val & \cellcolor{blue!60}Test  \\ \hline
Mikolov et al (2011)\cite{Mikolov11} & $163.2$  & $149.9$ \\ \hline
Zaremba et al (2014)\cite{DBLP:journals/corr/ZarembaSV14} & $82.62$  & $78.29$ \\ \hline
\end{tabular}
\end{center}
\end{figure}

\end{frame}

\begin{frame}{Novas arquiteturas: \url{https://arxiv.org/abs/1708.02182}}

\begin{center}
\includegraphics[scale=0.34]{images/SocherPTB.png}
\end{center}
\end{frame}

\begin{frame}
\begin{center}
\textbf{\LARGE{Obrigado!}}
\end{center}

\vspace{0.3cm}

\begin{center}
\includegraphics[scale=0.15]{images/gliclogo.pdf}
\end{center}
\url{https://glicusp.wordpress.com/}

\vspace{0.3cm}

\begin{center}
\includegraphics[scale=0.3]{images/logo0.pdf}
\end{center}
\url{https://www.ime.usp.br/~liamf/}

\end{frame}


\begin{frame}[allowframebreaks]{References}

  \bibliography{my_references}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}